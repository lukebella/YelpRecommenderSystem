{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Aggiungere cella per diminuire calcoli (e dimensione dataset?)\n",
    "- Creare un RDD i cui elementi siano le coppie righe/colonne e con un'operazione di map/reduce emettere coppie chiave-valore nelle quali la chiave indica la posizione (i, j) dell'elemento della matrice da aggiornare e il valore indica il nuovo elemento della matrice, utilizzando le formule del paragrafo 9.4.4. In questo modo \"parallelizzate\" gli aggiornamenti di tutti gli elementi della matrice. Eventualmente potete evitare di aggiornare d'un colpo tutta la matrice (potrebbe portare a difficoltà nella convergenza), decidendo in modo probabilistico ogni volta se aggiornare oppure no un elemento: basta che il job map/reduce generi un bit pseudocasuale che vale 1 con probabilità p, e se il suo valore è 1 allora utilizzate le formule di cui sopra, altrimenti restituite il valore attuale di U (fa parte della colonna che ottenete in input). In questo modo aggiornerete all'incirca una frazione pari a p dei valori della matrice. Lo stesso vale per V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    !pip install pyspark\n",
    "    !git clone https://github.com/lukebella/YelpRecommenderSystem.git\n",
    "    !mv YelpRecommenderSystem/* .\n",
    "    !rm -fr YelpRecommenderSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
    "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
    "!kaggle datasets download -p ./data -d yelp-dataset/yelp-dataset\n",
    "!unzip -n ./data/yelp-dataset.zip -d ./data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_filename = 'data/yelp_academic_dataset_review.json'\n",
    "user_filename = 'data/yelp_academic_dataset_user.json'\n",
    "business_filename = 'data/yelp_academic_dataset_business.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lavorare su un sample più piccolo (10%)\n",
    "### conviente farlo solo sulle review per problemi di join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .master().config('spark.driver.memory', \"15g\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# sc = pyspark.SparkContext().getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add docstring\n",
    "def from_json_to_RDD(filename):\n",
    "    raw_df = spark.read.json(filename)\n",
    "    raw_df.show()\n",
    "    #return raw_df.sample(frac=1/10, random_state = 0).dropna().rdd\n",
    "    return raw_df.rdd\n",
    "\n",
    "raw_review_RDD = from_json_to_RDD(review_filename)#.sample(True, 1, 0)  #699192 rows #shuffle\n",
    "raw_user_RDD = from_json_to_RDD(user_filename)\n",
    "raw_business_RDD = from_json_to_RDD(business_filename)\n",
    "#print(raw_business_RDD.filter(lambda x: x[2]=='bqFG0QJY9jj2m55OqAVHeA').first())\n",
    "\n",
    "raw_review_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample number of users and take all the reviews made by the users in the subset\n",
    "#TODO that not all the users taken have made a review, we could try to eliminate those who have not made a review? \n",
    "# Those users should see the starting recommendation...\n",
    "print(raw_user_RDD.count())\n",
    "raw_user_RDD = raw_user_RDD.filter(lambda x:x[-4]>0).sample(False, 1/500, 0) #withReplacement of .sample True or False?\n",
    "user_list = raw_user_RDD.map(lambda x: x[-2]).collect()\n",
    "#3295\n",
    "raw_review_RDD = raw_review_RDD.filter(lambda x: (x[8] in user_list))\n",
    "print(len(user_list))\n",
    "raw_review_RDD.count()\n",
    "#11444\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVE RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_tuple(entry):\n",
    "    \"\"\" Parse a row in the review dataset form pyspark.sql.Row to tuple (remove unintersted columns)\n",
    "    Args:\n",
    "        entry (pyspark.sql.types.Row): a row in the review dataset in JSON format\n",
    "    Returns:\n",
    "        tuple: (review_id, user_id, business_id, stars, useful, funny, cool, text)\n",
    "    \"\"\"\n",
    "\n",
    "    return (str(entry[\"review_id\"]),    # 0\n",
    "            str(entry[\"user_id\"]),      # 1\n",
    "            str(entry[\"business_id\"]),  # 2\n",
    "            int(entry[\"stars\"]),        # 3\n",
    "            int(entry[\"useful\"]),       # 4\n",
    "            int(entry[\"funny\"]),        # 5\n",
    "            int(entry[\"cool\"]),         # 6\n",
    "            str(entry[\"text\"]))         # 7\n",
    "\n",
    "\n",
    "review_RDD = raw_review_RDD.map(get_review_tuple)\n",
    "\n",
    "review_count = review_RDD.count()\n",
    "\n",
    "print(f'There are {review_count} reviews in the dataset')\n",
    "print(f'Reviews: {review_RDD.first()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_tuple(entry):\n",
    "    \"\"\" Parse a row in the user dataset form pyspark.sql.Row to tuple (remove unintersted columns)\n",
    "    Args:\n",
    "        entry (pyspark.sql.types.Row): a row in the user dataset in JSON format\n",
    "    Returns:\n",
    "        tuple: (user_id, name, review_count, average_stars, useful, funny, cool, fans)\n",
    "    \"\"\"\n",
    "\n",
    "    return (str(entry[\"user_id\"]),          # 0\n",
    "            str(entry[\"name\"]),             # 1\n",
    "            int(entry[\"review_count\"]),     # 2\n",
    "            float(entry[\"average_stars\"]),  # 3\n",
    "            int(entry[\"useful\"]),           # 4\n",
    "            int(entry[\"funny\"]),            # 5\n",
    "            int(entry[\"cool\"]),             # 6\n",
    "            int(entry[\"fans\"]))             # 7\n",
    "\n",
    "\n",
    "user_RDD = raw_user_RDD.map(get_user_tuple)\n",
    "\n",
    "user_count = user_RDD.count()\n",
    "\n",
    "print(f'There are {user_count} users in the dataset')\n",
    "print(f'Users: {user_RDD.first()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_tuple(entry):\n",
    "    \"\"\" Parse a row in the business dataset form pyspark.sql.Row to tuple (remove unintersted columns)\n",
    "    Args:\n",
    "        entry (pyspark.sql.types.Row): a row in the business dataset in JSON format\n",
    "    Returns:\n",
    "        tuple: (business_id, name, city, state, stars, review_count, categories)\n",
    "    \"\"\"\n",
    "\n",
    "    categories = [] if entry[\"categories\"] == None \\\n",
    "                    else str(entry[\"categories\"]).split(\", \")\n",
    "    \n",
    "    return (str(entry[\"business_id\"]),  # 0\n",
    "            str(entry[\"name\"]),         # 1\n",
    "            str(entry[\"city\"]),         # 2\n",
    "            str(entry[\"state\"]),        # 3\n",
    "            float(entry[\"stars\"]),      # 4\n",
    "            int(entry[\"review_count\"]), # 5\n",
    "            categories)                 # 6\n",
    "\n",
    "#TODO Attributes?\n",
    "\n",
    "business_RDD = raw_business_RDD.map(get_business_tuple)\n",
    "\n",
    "business_count = business_RDD.count()\n",
    "# print(business_RDD.filter(lambda x: x[0]=='1H8ReY5GlGcHJz7umVidkg').first())\n",
    "\n",
    "print(f'There are {business_count} business in the dataset')\n",
    "print(f'Business: {business_RDD.first()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate overall\n",
    "### - for each business_id:\n",
    "\n",
    "$$\n",
    "  \\Delta = \\dfrac{1}{2} \n",
    "              \\left( \n",
    "                  \\dfrac{\\text{useful} + \\dfrac{1}{2}(\\text{funny} + \\text{cool})} \n",
    "                        {\\text{best\\ useful} + \\dfrac{1}{2}(\\text{best\\ funny} + \\text{best\\ cool})}\n",
    "                        + \n",
    "                  \\dfrac{\\text{fans}}\n",
    "                        {\\text{best\\ fans}}\n",
    "              \\right)\n",
    "$$\n",
    "$$\n",
    "\\Delta : [0, 1]\n",
    "$$\n",
    "$$\n",
    "  \\text{overall} = \\begin{cases} \n",
    "              \\text{stars} + \\Delta & \\text{if stars } \\ge 3\\\\ \n",
    "              \\text{stars} - \\Delta & \\text{if stars } \\lt 3\n",
    "            \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix all comment\n",
    "\n",
    "#review_RDD.filter(lambda x: x[3]==0).count()\n",
    "\n",
    "\n",
    "#stars - useful - funny - cool - numero fan\n",
    "\n",
    "\n",
    "#if stars <3\n",
    "    #stars - {[useful + (funny + cool)/2]/[best useful + (best funny + best cool)/2] + (numero fan/best user fans)}/2\n",
    "#else\n",
    "    #stars + {[useful + (funny + cool)/2]/[best useful + (best funny + best cool)/2] + (numero fan/best user fans)}/2\n",
    "\n",
    "    #dati mancanti: numero fan; best fan; best useful/funny/cool per ristorante\n",
    "#rdd_test_ufc = (id ristorsante, best useful, best funny, best cool)\n",
    "#best fan = query su dataset utenti --> user_RDD.max().first()\n",
    "\n",
    "#review_id - user_id - id_rist - overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_overall_rating: function that generates the bias in order to incentivate or decrease the importance or overall rating of a review\n",
    "\n",
    "# TODO fix function\n",
    "\n",
    "#+ (fans/best_user_fans)) / 2\n",
    "\n",
    "# def get_overall_rating(stars, best_useful, best_funny, best_cool, useful, funny, cool):\n",
    "#     overall = (useful + (funny + cool)/2) / (best_useful + (best_funny + best_cool)/2) \n",
    "#     if stars < 3:\n",
    "#         return stars - overall\n",
    "#     else:\n",
    "#         return stars + overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_id_useful_funny_cool(row):\n",
    "    \"\"\"\n",
    "        Writes a tuple with key business_id and attributes useful, funny, cool\n",
    "    \"\"\"\n",
    "    return (row[2], (row[4], row[5], row[6]))\n",
    "\n",
    "def get_max_useful_funny_cool(row1, row2):\n",
    "    \"\"\"\n",
    "        Returns a new RDD with, for each business_id key, the maxima values of useful, funny, cool\n",
    "    \"\"\"\n",
    "    return tuple(max(row1[i], row2[i]) for i in range(3))\n",
    "\n",
    "review_tuple_RDD = review_RDD.map(get_business_id_useful_funny_cool)\n",
    "review_best_ufc_RDD = review_tuple_RDD.reduceByKey(get_max_useful_funny_cool)\n",
    "\n",
    "review_best_ufc_RDD.take(5)\n",
    "\n",
    "# review_best_ufc_RDD tuple: (business_id, (useful, funny, cool)): for each reastaurant, this tuple takes the maxima values of useful, funny and cool. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_review_and_best_ufc_RDD(row):\n",
    "    \"\"\"\n",
    "        Merge, for each business_id, the row values with the best useful, funny, cool ones\n",
    "    \"\"\"\n",
    "    return (row[2], (row[0], row[1], row[3], row[4], row[5], row[6]))\n",
    "\n",
    "review_ufc_RDD = review_RDD.map(rearrange_review_and_best_ufc_RDD).join(review_best_ufc_RDD)\n",
    "review_ufc_RDD.first()\n",
    "\n",
    "# (id_business, ((id_review, id_user, star, useful, funny , cool), (best useful, best funny, best cool)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_overall(x):\n",
    "    \"\"\"\n",
    "        Rearrange review_ufc_RDD and calculates the partial overall of the delta above\n",
    "        partial_overall ={[useful + (funny + cool)/2]/[best useful + (best funny + best cool)/2]\n",
    "    \"\"\"\n",
    "    key = (x[1][0][0], x[1][0][1], x[0], x[1][0][2])\n",
    "    num = (x[1][0][3] + (x[1][0][4] + x[1][0][5])/2)\n",
    "    den = (x[1][1][0] + (x[1][1][1] + x[1][1][2])/2)\n",
    "    return (*key,(num/den if sum(x[1][1]) != 0 else 0))\n",
    "\n",
    "\n",
    "partial_review_overall_RDD = review_ufc_RDD.map(partial_overall)\n",
    "partial_review_overall_RDD.first()\n",
    "\n",
    "# tuple: (review_id, user_id, buisness_id, stars, partial overall) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple: (user_id, fans)\n",
    "user_fans_RDD = user_RDD.map(lambda x: (x[0], x[7]))\n",
    "\n",
    "# TODO add map to turn the result in a simple tuple\n",
    "# tuple: (user_id, (review_id, buisness_id, stars, partial overall), fans)\n",
    "partial_overall_fans_RDD = partial_review_overall_RDD.map(lambda x: (x[1], (x[0], x[2], x[3], x[4]))).join(user_fans_RDD)\n",
    "\n",
    "partial_overall_fans_RDD.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user with the highest number of fans\n",
    "best_user_fans = user_RDD.max(lambda x: x[7])\n",
    "print(\"User with the highest number of fans: \",best_user_fans)\n",
    "\n",
    "# TODO if best user fans = 0\n",
    "\n",
    "def review_overall(x):\n",
    "    user_id, review_id, business_id, stars= x[0], x[1][0][0], x[1][0][1], x[1][0][2]\n",
    "    value = stars + (x[1][0][3]+x[1][1]/best_user_fans[7])/(2 if stars >=3 else -2) \n",
    "    return (review_id, user_id, business_id, value)\n",
    "\n",
    "    #tuple: (review_id, user_id, business_id, stars + (partial overall + fans/best_fans))\n",
    "\n",
    "\n",
    "review_overall_RDD = partial_overall_fans_RDD.map(review_overall)\n",
    "\n",
    "review_overall_RDD.sortBy(lambda x:x[3], ascending=False).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping businesses that have at least 20 reviews\n",
    "\n",
    "#review_by_business_filtered_RDD = review_overall_RDD.map(lambda x: (x[2], (x[0], x[1], x[3]))).groupByKey()\\\n",
    "#                                                    .mapValues(list).filter(lambda x: len(x[1])>=20)\n",
    "# review_by_business_filtered_RDD.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFunction(tuple):\n",
    "    \"\"\" Construct the sort string (does not perform actual sorting)\n",
    "    Args:\n",
    "        tuple: (rating, MovieName)\n",
    "    Returns:\n",
    "        sortString: the value to sort with, 'rating MovieName'\n",
    "    \"\"\"\n",
    "    #key = unicode('%.3f' % tuple[0])\n",
    "    if (tuple[1][1]!= None):\n",
    "        value = '{:.3f}'.format(tuple[1][1])\n",
    "    else:\n",
    "        value = ''\n",
    "    key = tuple[0]\n",
    "    return (value + ' ' + key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make, for each business, the average of the stars for all the reviews\n",
    "\n",
    "# business_overall_RDD = review_by_business_filtered_RDD.map(lambda x: (x[0],  sum(i[2] for i in x[1])/len(x[1])))\n",
    "# business_overall_RDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top recommended businesses\n",
    "#business_RDD.map(lambda x: (x[0], (x[1], x[6]))).leftOuterJoin(business_overall_RDD).sortBy(sortFunction, False).take(10)  #x[2], x[3],.filter(lambda x: 'Restaurants' in x[1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_overall_bis = review_overall_RDD.map(lambda x: (x[1], x[2], x[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Distance\n",
    "def cosine_dist(value):\n",
    "    \"\"\" \n",
    "    arg: utente da confrontare\n",
    "    per ogni utente:\n",
    "        - user_id, prodotto tra rating di item per stessa chiave di item, lista di rating\n",
    "        - somma di valori nel numeratore, somma di liste del denominatore\n",
    "        - prodotto delle somma quadratiche nel denominatore tra i due utenti\n",
    "        - prendi utenti che hanno qualche voto su un business in comune diverso da zero\n",
    "    \"\"\"\n",
    "    value_review = dict(review_overall_bis.filter(lambda x:x[0] == value).map(lambda x:(x[1],x[2])).collect()) #business: voto\n",
    "    user_sim = review_overall_bis.map(lambda x: (x[0], (0 if x[1] not in value_review.keys() \\\n",
    "                                                        else x[2]*value_review[x[1]], [x[2]])))\\\n",
    "                                 .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "                                 .map(lambda x:(x[0], x[1][0]/((sum(i**2 for i in x[1][1])**0.5) * \\\n",
    "                                                               (sum(i**2 for i in value_review.values())**0.5))))\\\n",
    "                                 .filter(lambda x:x[1]!=0 and x[0]!=value).take(5)\n",
    "    return user_sim, value_review\n",
    "\n",
    "def collaborative_filtering(value):\n",
    "    print(value)\n",
    "    user_sim, value_review = cosine_dist(value)\n",
    "    print(value_review.keys())\n",
    "    return review_overall_bis.filter(lambda x: (x[0] in dict(user_sim).keys() and x[1] not in value_review.keys()))\\\n",
    "                             .sortBy(lambda x:x[2],ascending = False).take(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(review_overall_bis.take(10)[5])\n",
    "business_collab = collaborative_filtering('onQsolCnpHwtOZCIuI91vQ')\n",
    "# print(review_overall_RDD.filter(lambda x:x[1]=='Eep1pCr1zMqub16tYgUFgQ').collect())\n",
    "business_collab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UV Decompostion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO in the collaborative prt, take into consideration the categories and the State\n",
    "#TODO based on the number of reviews, multiply by the percentage of the reviews made in a particular state (same for the categories)\n",
    "# print(dict(sorted(business_RDD.flatMap(lambda x: tuple(x[6])).countByValue().items(), key=lambda x: x[1], reverse=True))) lista delle categorie più frequenti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Because the differences in the quality of items and the rating scales of users are\n",
    "# such important factors in determining the missing elements of the matrix M , it\n",
    "# is often useful to remove these influences before doing anything else. The idea\n",
    "# was introduced in Section 9.3.1. We can subtract from each nonblank element\n",
    "# mij the average rating of user i. Then, the resulting matrix can be modified\n",
    "# by subtracting the average rating (in the modified matrix) of item j. It is also\n",
    "# possible to first subtract the average rating of item j and then subtract the\n",
    "# average rating of user i in the modified matrix. The results one obtains from\n",
    "# doing things in these two different orders need not be the same, but will tend\n",
    "# to be close. A third option is to normalize by subtracting from mij the average\n",
    "# of the average rating of user i and item j, that is, subtracting one half the sum\n",
    "# of the user average and the item average.\n",
    "# If we choose to normalize M , then when we make predictions, we need to\n",
    "# undo the normalization. That is, if whatever prediction method we use results\n",
    "# in estimate e for an element mij of the normalized matrix, then the value\n",
    "# we predict for mij in the true utility matrix is e plus whatever amount was\n",
    "# subtracted from row i and from column j during the normalization process.\n",
    "\n",
    "\n",
    "\n",
    "#We have chosen to implement the third option\n",
    "#review_overall_RDD.flatMap(lambda x : ((x[1], x[2]), 1)).reduceByKey(lambda x, y : x + y).sortBy(lambda x: x[1], False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#media voti utente\n",
    "#average_user_RDD = review_overall_RDD.map(lambda x: (x[1], x[3])).reduceByKey(lambda x,y: (x+y)/2)\n",
    "# average_user_RDD.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#media voti business\n",
    "#\n",
    "#average_business_RDD = review_overall_RDD.map(lambda x: (x[2], x[3])).reduceByKey(lambda x,y: (x+y)/2)\n",
    "# average_business_RDD.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge review rates with average user\n",
    "#partial_normalized_RDD = review_overall_RDD.map(lambda x: (x[1], (x[0], x[2], x[3]))).join(average_user_RDD).map(lambda x: (x[1][0][0], x[0], x[1][0][1], x[1][0][2], x[1][1]))\n",
    "# partial_normalized_RDD.take(10)\n",
    "\n",
    "# tuple: (review_id, user_id, buisness_id, overall, average_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_RDD = partial_normalized_RDD.map(lambda x: (x[1], (x[0], x[2], x[3]))) \\\n",
    "#                                        .join(average_business_RDD) \\\n",
    "#                                        .map(lambda x: ((x[1][0][0], x[0]), (x[1][0][1] - (x[1][1]+x[1][0][2])/2))) \\\n",
    "#                                        .reduceByKey(lambda x, y : (x+y)/2) \\\n",
    "#                                        .map(lambda x: (*x[0], x[1]))\n",
    "\n",
    "# normalized_RDD.first()\n",
    "# # tuple: (user_id, buisness_id, overall - normalized_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to train and predict the vectors U and V\n",
    "# train(trainingRDD, rank, iterations, regularization_parameter)  --> (U,V)\n",
    "#We need to calculate RMSE between the utility and our matrix\n",
    "\n",
    "#TODO initialize U and V with the overall average value\n",
    "\n",
    "rank = 2\n",
    "# regularization_parameter = 0.1\n",
    "\n",
    "U_RDD = user_RDD.map(lambda x: (x[0], [1 for _ in range(rank)]))#.sortBy(lambda x: x[0])\n",
    "V_RDD = business_RDD.map(lambda x: (x[0], [1 for _ in range(rank)]))#.sortBy(lambda x: x[0])\n",
    "\n",
    "# U_RDD.take(5)\n",
    "# V_RDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_overall_bis = review_overall_RDD.map(lambda x: (x[1], x[2], x[3]))\n",
    "#review_overall_bis = review_RDD.map(lambda x: (x[1], x[2], x[3]))\n",
    "\n",
    "#print(review_overall_RDD.filter(lambda x: x[1]=='cxuxXkcihfCbqt5Byrup8Q').first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE We are bypassing at the moment the validation set (useful for achieving the best rank)\n",
    "trainingRDD, testRDD = review_overall_bis.randomSplit([7,3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_BC = sc.broadcast(trainingRDD.collect())\n",
    "\n",
    "# list(filter(lambda x: x[0]=='cxuxXkcihfCbqt5Byrup8Q', M_BC.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo no map\n",
    "# V_BC = sc.broadcast(dict(V_RDD.map(lambda x : (x[0], x[1])).collect()))\n",
    "V_BC = sc.broadcast(dict(V_RDD.collect()))\n",
    "#V_BC.value['KsFuzQCOPhP8eHmt3rP11Q']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo no map\n",
    "# U_BC = sc.broadcast(dict(U_RDD.map(lambda x : (x[0], x[1])).collect()))\n",
    "U_BC = sc.broadcast(dict(U_RDD.collect()))\n",
    "\n",
    "#len(U_BC.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized-->tuple: (user_id, buisness_id, overall - normalized_value)\n",
    "# review_overall --> tuple: (review_id, user_id, business_id, stars + (partial overall + fans/best_fans))\n",
    "#print(business_RDD.filter(lambda x: x[0]=='1H8ReY5GlGcHJz7umVidkg').first())\n",
    "\n",
    "def update_U(entry):\n",
    "    id = entry[0]\n",
    "    U_value = entry[1]\n",
    "    reviews = [i for i in M_BC.value if i[0] == id]\n",
    "    for s in range(rank):\n",
    "        res = 0\n",
    "        den = 0\n",
    "        for review in reviews:\n",
    "            j = review[1] # business_id\n",
    "            p = 0\n",
    "            for k in [i for i in range(rank) if i != s]:\n",
    "                #print(V_BC.value)\n",
    "                \n",
    "                p += U_value[k] * V_BC.value[j][k]\n",
    "\n",
    "            res += V_BC.value[j][s] * (review[2] - p)\n",
    "            den += V_BC.value[j][s]**2\n",
    "        U_value[s] = res / (den if den > 0 else 1)\\\n",
    "            #if len(reviews)>0 else 1\n",
    "\n",
    "    return (id, U_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_V(entry):\n",
    "    id = entry[0]\n",
    "    V_value = entry[1]\n",
    "    reviews = [i for i in M_BC.value if i[1] == id]\n",
    "\n",
    "    # out = [1]*rank\n",
    "    for s in range(rank):\n",
    "        res = 0\n",
    "        den = 0\n",
    "        for review in reviews:\n",
    "            j = review[0] # user_id\n",
    "            p = 0\n",
    "            for k in [i for i in range(rank) if i != s]:\n",
    "                p += V_value[k] * U_BC.value[j][k]\n",
    "\n",
    "            res += U_BC.value[j][s] * (review[2] - p)\n",
    "            den += U_BC.value[j][s]**2\n",
    "        V_value[s] = (res / (den if den > 0 else 1))\\\n",
    "#            if len(reviews)>0 else 1\n",
    "\n",
    "    return (id, V_value)\n",
    "\n",
    "\n",
    "# V_RDD.map(lambda x :  update_V(x)).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(U_RDD,V_RDD,iterations):  \n",
    "# iterations = 2\n",
    "    global V_BC, U_BC\n",
    "    for i in range(iterations):\n",
    "        V_BC = sc.broadcast(dict(V_RDD.collect()))\n",
    "        U_RDD = U_RDD.map(update_U)\n",
    "\n",
    "        U_BC = sc.broadcast(dict(U_RDD.collect()))\n",
    "        V_RDD = V_RDD.map(update_V)\n",
    "\n",
    "    return U_RDD.collect(), V_RDD.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,V = train(U_RDD, V_RDD, iterations =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take an user-id and suggest business that he has not reviewed\n",
    "\n",
    "def predict(u_value, business_value):\n",
    "    return sum(u_value[i]* business_value[i] for i in range(len(u_value)))\n",
    "\n",
    "\n",
    "def print_prediction(username, business_name):\n",
    "\n",
    "    U_dict = dict(U)\n",
    "    V_dict = dict(V)\n",
    "    un = user_RDD.filter(lambda x: x[0]== username).first()[1]\n",
    "    bn = business_RDD.filter(lambda x: x[0]== business_name).first()[2]\n",
    "    print(\"User:\",un, \"----- Business:\",bn, \":\",predict(U_dict[username], V_dict[business_name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction(\"DEFTCj_6zHCAtwyUvNEBZQ\", \"rBmpy_Y1UbBx8ggHlyb7hA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE of UV Decomposition\n",
    "\n",
    "def rmse(testRDD):\n",
    "    rmse = 0\n",
    "    U_dict = dict(U)\n",
    "    V_dict = dict(V)\n",
    "    for i in testRDD.collect():\n",
    "        rmse += (i[2] - predict(U_dict[i[0]], V_dict[i[1]]))**2\n",
    "    return (rmse/len(M_BC.value))**0.5\n",
    "\n",
    "print(\"RMSE:\", rmse(testRDD))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to populate U and V with the average value of review rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_value = review_overall_bis.map(lambda x: (1, x[2])).reduceByKey(lambda x,y:(x+y)/2).first()[1]\n",
    "avg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_RDD = user_RDD.map(lambda x: (x[0], [avg_value for _ in range(rank)]))#.sortBy(lambda x: x[0])\n",
    "V_RDD = business_RDD.map(lambda x: (x[0], [avg_value for _ in range(rank)]))#.sortBy(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,V = train(U_RDD, V_RDD, iterations =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE of UV Decomposition\n",
    "print(\"RMSE:\", rmse(testRDD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash(s):\n",
    "    h = hashlib.sha1(s.encode())\n",
    "    return int(h.hexdigest(),16)%((2**31)-1)\n",
    "\n",
    "review_overall_tris = review_overall_bis.map(lambda x: (hash(x[0]), hash(x[1]), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRDD, validationRDD, testRDD = review_overall_tris.randomSplit([6, 2, 2], seed=0)\n",
    "\n",
    "print('Training: %s, validation: %s, test: %s\\n' % (trainingRDD.count(),\n",
    "                                                    validationRDD.count(),\n",
    "                                                    testRDD.count()))\n",
    "print(trainingRDD.take(3))\n",
    "print(validationRDD.take(3))\n",
    "print(testRDD.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def computeError(predictedRDD, actualRDD):\n",
    "    \"\"\" Compute the root mean squared error between predicted and actual\n",
    "    Args:\n",
    "        predictedRDD: predicted ratings for each movie and each user where each entry is in the form\n",
    "                      (UserID, MovieID, Rating)\n",
    "        actualRDD: actual ratings where each entry is in the form (UserID, MovieID, Rating)\n",
    "    Returns:\n",
    "        RSME (float): computed RSME value\n",
    "    \"\"\"\n",
    "    # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    predictedReformattedRDD = predictedRDD.map(lambda i: ((i[0], i[1]), i[2]))\n",
    "\n",
    "    # Transform actualRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    actualReformattedRDD = actualRDD.map(lambda i: ((i[0], i[1]), i[2]))\n",
    "\n",
    "    # Compute the squared error for each matching entry (i.e., the same (User ID, Movie ID) in each\n",
    "    # RDD) in the reformatted RDDs using RDD transformtions - do not use collect()\n",
    "    squaredErrorsRDD = (predictedReformattedRDD\n",
    "                        .join(actualReformattedRDD)\n",
    "                        .map(lambda i: math.pow(i[1][0] - i[1][1], 2))\n",
    "                       )\n",
    "\n",
    "    # Compute the total squared error - do not use collect()\n",
    "    totalError = squaredErrorsRDD.reduce(lambda a, b: a+b)\n",
    "\n",
    "    # Count the number of entries for which you computed the total squared error\n",
    "    numRatings = squaredErrorsRDD.count()\n",
    "\n",
    "    # Using the total squared error and the number of entries, compute the RSME\n",
    "    return math.pow(float(totalError) / numRatings, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "validationForPredictRDD = validationRDD.map(lambda i: (i[0], i[1]))\n",
    "\n",
    "seed = 5\n",
    "iterations = 5\n",
    "regularizationParameter = 0.1\n",
    "ranks = [2, 4, 8, 12]\n",
    "errors = [0, 0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "minError = float('inf')\n",
    "bestRank = -1\n",
    "bestIteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingRDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularizationParameter)\n",
    "    predictedRatingsRDD = model.predictAll(validationForPredictRDD)\n",
    "    error = computeError(predictedRatingsRDD, validationRDD)\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print('For rank {} the RMSE is {}'.format(rank, error))\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = rank\n",
    "\n",
    "print('The best model was trained with rank {}'.format(bestRank))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
