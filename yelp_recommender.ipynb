{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    !git clone https://github.com/lukebella/YelpRecommenderSystem.git\n",
    "    !mv YelpRecommenderSystem/* .\n",
    "    !rm -fr YelpRecommenderSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
    "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
    "!kaggle datasets download -p ./data -d yelp-dataset/yelp-dataset\n",
    "!unzip -n ./data/yelp-dataset.zip -d ./data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_filename = 'data/yelp_academic_dataset_review.json'\n",
    "user_filename = 'data/yelp_academic_dataset_user.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .master().config('spark.driver.memory', \"15g\")\n",
    "# sc = pyspark.SparkContext().getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add docstring\n",
    "def from_json_to_RDD(filename):\n",
    "    raw_df = spark.read.json(filename)\n",
    "    # raw_df.printSchema()\n",
    "    raw_df.show()\n",
    "    return raw_df.rdd\n",
    "\n",
    "raw_review_RDD = from_json_to_RDD(review_filename)\n",
    "raw_user_RDD = from_json_to_RDD(user_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_tuple(entry):\n",
    "    \"\"\" Parse a line in the review dataset form pyspark.sql.Row to tuple\n",
    "    Args:\n",
    "        entry (pyspark.sql.Row): a line in the review dataset in JSON format\n",
    "    Returns:\n",
    "        tuple: (review_id, user_id, business_id, stars, useful, funny, cool, text)\n",
    "    \"\"\"\n",
    "\n",
    "    return (str(entry[\"review_id\"]),    # 0\n",
    "            str(entry[\"user_id\"]),      # 1\n",
    "            str(entry[\"business_id\"]),  # 2\n",
    "            int(entry[\"stars\"]),        # 3\n",
    "            int(entry[\"useful\"]),       # 4\n",
    "            int(entry[\"funny\"]),        # 5\n",
    "            int(entry[\"cool\"]),         # 6\n",
    "            str(entry[\"text\"]))         # 7\n",
    "\n",
    "\n",
    "review_RDD = raw_review_RDD.map(get_review_tuple)\n",
    "\n",
    "review_count = review_RDD.count()\n",
    "\n",
    "print(f'There are {review_count} reviews in the dataset')\n",
    "print(f'Reviews: {review_RDD.take(3)}') # TODO fix print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix docstring\n",
    "def get_user_tuple(entry):\n",
    "    \"\"\" Parse a line in the review dataset form pyspark.sql.Row to tuple\n",
    "    Args:\n",
    "        entry (pyspark.sql.Row): a line in the review dataset in JSON format\n",
    "    Returns:\n",
    "        tuple: (review_id, user_id, business_id, stars, useful, funny, cool, text)\n",
    "    \"\"\"\n",
    "\n",
    "    return (str(entry[\"user_id\"]),          # 0\n",
    "            str(entry[\"name\"]),             # 1\n",
    "            int(entry[\"review_count\"]),     # 2\n",
    "            float(entry[\"average_stars\"]),  # 3\n",
    "            int(entry[\"useful\"]),           # 4\n",
    "            int(entry[\"funny\"]),            # 5\n",
    "            int(entry[\"cool\"]),             # 6\n",
    "            int(entry[\"fans\"]))             # 7\n",
    "\n",
    "\n",
    "user_RDD = raw_user_RDD.map(get_user_tuple)\n",
    "\n",
    "user_count = user_RDD.count()\n",
    "\n",
    "print(f'There are {user_count} users in the dataset')\n",
    "print(f'Users: {user_RDD.take(3)}') # TODO fix print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_RDD.filter(lambda x: x[3]==0).count()\n",
    "\n",
    "\n",
    "#stars - useful - funny - cool - numero fan\n",
    "\n",
    "\n",
    "#if stars <3\n",
    "    #stars - {[useful + (funny + cool)/2]/[best useful + (best funny + best cool)/2] + (numero fan/best user fans)}/2\n",
    "#else\n",
    "    #stars + {[useful + (funny + cool)/2]/[best useful + (best funny + best cool)/2] + (numero fan/best user fans)}/2\n",
    "\n",
    "    #dati mancanti: numero fan; best fan; best useful/funny/cool per ristorante\n",
    "#rdd_test_ufc = (id ristorsante, best useful, best funny, best cool)\n",
    "#best fan = query su dataset utenti --> user_RDD.max().first()\n",
    "\n",
    "#review_id - user_id - id_rist - overall\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_best_ufc_RDD tuple: (business_id, (useful, funny, cool)): for each reastaurant, this tuple takes the maxima values of useful, funny and cool. \n",
    "review_best_ufc_RDD = review_RDD.map(lambda x : (x[2], (x[4], x[5], x[6]))).reduceByKey(lambda x, y : tuple(max(x[i], y[i]) for i in range(len(y))))\n",
    "\n",
    "review_best_ufc_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_user_fan = user_RDD.max(lambda x:x[7])\n",
    "print(best_user_fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_overall_rating: function that generates the bias in order to incentivate or decrease the importance or overall rating of a review\n",
    "\n",
    "def get_overall_rating(stars, best_useful, best_funny, best_cool, useful, funny, cool, fans):\n",
    "    overall = ((useful + (funny + cool)/2) / (best_useful + (best_funny + best_cool)/2) + (fans/best_user_fan)) /2\n",
    "    if stars < 3:\n",
    "        return stars - overall\n",
    "    else: \n",
    "        return stars + overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fan_RDD = user_RDD.map(lambda x: (x[0],x[7]))\n",
    "review_fans_RDD = review_RDD.map(lambda x: (x[1], x[0], x[2], x[3], x[4], x[5], x[6])).join(user_fan_RDD)\n",
    "review_fans_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_fan_RDD.join(review_RDD.map(lambda x: (x[1], (x[0], x[2], x[3], x[4], x[5], x[6])))).first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
